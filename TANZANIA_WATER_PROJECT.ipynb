{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TANZANIA WATER PROJECT",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stogaja/Tanzanian-Water-Project/blob/main/TANZANIA_WATER_PROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. **Defining the Question** "
      ],
      "metadata": {
        "id": "kyMiGNEUlmf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tanzania is the largest country in East Africa, with a population of 52 million people. But of those 52 million people, 23 million have no choice but to drink dirty water from unsafe sources. 44 million do not have access to adequate sanitation and 4000 children die from preventable diseases due to unsafe water. Safe water is scarce, and often women and children have to spend two to seven hours collecting clean water (WaterAid, 2016). This is quite the predicament. Water is a basic need and right for all human beings. The Tanzanian Ministry of water agrees and together with Taarifa, they aim to improve sanitation conditions in their country.\n",
        "Water is fundamental to life and the environment; it plays a central role in both, economic and social development activities. Water touches all the spheres of human life including domestic, livestock, fisheries, wildlife, industry and energy, recreation, and other social—economic activities. It plays a pivotal role in poverty alleviation through the enhancement of food security, domestic hygiene, and the environment. The availability of safe and clean water raises the standard of living while its inadequacy of it poses serious health risks and leads to a decline in the living standards and life expectancy. Major fresh water sources in Tanzania include lakes, rivers, streams, dams, and groundwater. However, these are not well distributed all over the country. Some areas lack both surface and groundwater sources. Increasing population growth and urbanization pose serious pressure on the quantity and quality of available water. The sustainability of the present and future human life and environment depends mainly on proper water resources management. \n"
      ],
      "metadata": {
        "id": "Gh2y2b2ZzQ7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a) Specifying the Question"
      ],
      "metadata": {
        "id": "9IVK5u0WrNuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Water supply to different parts of Tanzania is mainly done through pipes dug underground, while this is an initiative to curb the water problem, over 24 million people are still impacted by the crisis, that’s almost half of the population. This has resulted in poor sanitation, lack of safe drinking water as well as overcrowding at water sources, the adverse effects include disease outbreaks and generally very slow economic growth. The project aims to solve these problems by predicting which pipes are operating well, which ones need repairs and which ones are not working at all, as optimally functioning pipes will mean smooth delivery of water to where its needed."
      ],
      "metadata": {
        "id": "9DaUb6y0zUDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) Defining the Metric for Success"
      ],
      "metadata": {
        "id": "0kx_TWhLrqRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project will be considered a success when we can classify pumps into 3 categories namely:\n",
        "\n",
        "* functional : the waterpoint is operational and there are no repairs needed\n",
        "\n",
        "* functional needs repair : the waterpoint is operational, but needs repairs\n",
        "\n",
        "* non functional : the waterpoint is not operational"
      ],
      "metadata": {
        "id": "rcgdNTTmF4nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c) Understanding the context"
      ],
      "metadata": {
        "id": "K-Bo26x_sCm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### d) Recording the Experimental Design"
      ],
      "metadata": {
        "id": "UbeNftYisM_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## e) Data Relevance"
      ],
      "metadata": {
        "id": "Cbbw2IJCsfC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data has been proven to be valid and was provided by the Tanzania Water Ministry"
      ],
      "metadata": {
        "id": "WUDpcOgqGrVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Importing Libraries.**"
      ],
      "metadata": {
        "id": "jh70oWW6t0Zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install category_encoders"
      ],
      "metadata": {
        "id": "VQPG7LbXXVtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary libraries\n",
        "#\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import category_encoders as ce\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "VM-ZTQ7OYSRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **3. Reading the Data**"
      ],
      "metadata": {
        "id": "6E0MRfT2t9jF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Accessing working directory\n",
        "os.chdir('/content/drive/Shared drives/Final Project Group 2')"
      ],
      "metadata": {
        "id": "WBoOO9IUsxeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the csv files\n",
        "train_labels = pd.read_csv(\"Training set labels.csv\")\n",
        "train_values = pd.read_csv(\"Training set values.csv\")\n",
        "test_labels = pd.read_csv(\"SubmissionFormat.csv\")\n",
        "test_values = pd.read_csv(\"Test set values.csv\")"
      ],
      "metadata": {
        "id": "TcyQbazaYd5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the Training dataset\n",
        "training_data = train_labels.merge(train_values, left_on='id', right_on='id')\n",
        "training_data.head()"
      ],
      "metadata": {
        "id": "LdIeD1Sit_Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data.shape"
      ],
      "metadata": {
        "id": "qS6s6YRdwcHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the Testing dataset\n",
        "testing_data = test_labels.merge(test_values, left_on='id', right_on='id')\n",
        "testing_data.head()"
      ],
      "metadata": {
        "id": "D7ADYjz0vT0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_data.shape"
      ],
      "metadata": {
        "id": "5Tio8zlGwVJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining training and testing dataset\n",
        "# For purposes of cleaning and EDA\n",
        "dataset = pd.concat([training_data, testing_data], ignore_index=True, sort=False)\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "v3Yn-hv3voPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "id": "klfzDsSsxcMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the dataset\n",
        "dataset.to_csv(\"merged_dataset.csv\")"
      ],
      "metadata": {
        "id": "z8lIqcVlxtZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the dataset\n",
        "df = pd.read_csv(\"merged_dataset.csv\")"
      ],
      "metadata": {
        "id": "2ofp-EMXy3PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring the data"
      ],
      "metadata": {
        "id": "UrRVJKWYcVIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking for shape \n",
        "# size of the dataset\n",
        "print(\"The dataset consist of\",df.shape[0], \"rows and\", df.shape[1], \"columns\")"
      ],
      "metadata": {
        "id": "WNaNGtovchVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#a preview of the data \n",
        "df.head()"
      ],
      "metadata": {
        "id": "LibpqhtLcFHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking for colum names\n",
        "df.columns"
      ],
      "metadata": {
        "id": "OrqWNfFsdenB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* amount_tsh : Total static head (amount water available to waterpoint)\n",
        "\n",
        "* date_recorded : The date the row was entered\n",
        "\n",
        "* funder : Who funded the well\n",
        "\n",
        "* gps_height : Altitude of the well\n",
        "\n",
        "* installer : Organization that installed the well\n",
        "* longitude : GPS coordinate\n",
        "\n",
        "* latitude : GPS coordinate\n",
        "\n",
        "* wpt_name : Name of the waterpoint if there is one\n",
        "\n",
        "* num_private :Private use or not\n",
        "\n",
        "* basin : Geographic water basin\n",
        "\n",
        "* subvillage : Geographic location\n",
        "\n",
        "* region : Geographic location\n",
        "\n",
        "* region_code : Geographic location (coded)\n",
        "\n",
        "* district_code : Geographic location (coded)\n",
        "\n",
        "* lga : Geographic location\n",
        "\n",
        "* ward : Geographic location\n",
        "\n",
        "* population : Population around the well\n",
        "\n",
        "* public_meeting : True/False\n",
        "\n",
        "* recorded_by : Group entering this row of data\n",
        "\n",
        "* scheme_management : Who operates the waterpoint\n",
        "\n",
        "* scheme_name : Who operates the waterpoint\n",
        "\n",
        "* permit : If the waterpoint is permitted\n",
        "\n",
        "* construction_year : Year the waterpoint was constructed\n",
        "\n",
        "* extraction_type : The kind of extraction the waterpoint uses\n",
        "\n",
        "* extraction_type_group : The kind of extraction the waterpoint uses\n",
        "\n",
        "* extraction_type_class : The kind of extraction the waterpoint uses\n",
        "\n",
        "* management : How the waterpoint is managed\n",
        "\n",
        "* management_group : How the waterpoint is managed\n",
        "\n",
        "* payment : What the water costs\n",
        "\n",
        "* payment_type : What the water costs\n",
        "\n",
        "* water_quality : The quality of the water\n",
        "\n",
        "* quality_group : The quality of the water\n",
        "\n",
        "* quantity : The quantity of water\n",
        "quantity_group : The quantity of water\n",
        "\n",
        "* source : The source of the water\n",
        "\n",
        "* source_type : The source of the water\n",
        "\n",
        "* source_class : The source of the water\n",
        "\n",
        "* waterpoint_type : The kind of waterpoint\n",
        "\n",
        "* waterpoint_type_group : The kind of waterpoint"
      ],
      "metadata": {
        "id": "BgREt2WtHWHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cheking for data types if each columns \n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "vYDVZl0edr-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **4. Data Preparation**"
      ],
      "metadata": {
        "id": "vNKNFCd6f-kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning."
      ],
      "metadata": {
        "id": "F772tvW8gapS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a)Validity"
      ],
      "metadata": {
        "id": "hsug3SYpg1dL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview sample of 100 records to see whether all records are appropiately ordered\n",
        "df.sample(10)"
      ],
      "metadata": {
        "id": "oW-6E9YFgpCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping unnecesary columns\n",
        "df = df.drop(columns=\"Unnamed: 0\")"
      ],
      "metadata": {
        "id": "NwOr_pjJzjNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c) Uniformity"
      ],
      "metadata": {
        "id": "AfeQWsZIE3CV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking if columns are properly named \n",
        "df.columns"
      ],
      "metadata": {
        "id": "XrG6WadWFLgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Columns have uniform naming."
      ],
      "metadata": {
        "id": "Yh0cs7_Fosa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### d) Completeness"
      ],
      "metadata": {
        "id": "LPJr06NrhuF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here we check for missing values \n",
        "# Dealing with missing values \n",
        "# Checking the mumber of missing values by column and sorting for the smallest\n",
        "\n",
        "Total = df.isnull().sum().sort_values(ascending=False)\n",
        "\n",
        "# Calculating percentages\n",
        "percent_1 = df.isnull().sum()/df.isnull().count()*100\n",
        "\n",
        "# rounding off to one decimal point\n",
        "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
        "\n",
        "# creating a dataframe to show the values\n",
        "missing_data = pd.concat([Total, percent_2], axis=1, keys=['Total', '%'])\n",
        "missing_data"
      ],
      "metadata": {
        "id": "Uxyhx-Och3hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's replace the 0 from the construction year  with and arbitrarily selected year 1993\n",
        "\n",
        "df['construction_year'] = df['construction_year'].replace({0:1993})\n",
        "df['age'] = df['date_recorded'].astype(str).str[:4].astype(int) - df['construction_year']\n",
        "df['pop/year'] = df['population'].replace({0:1}) / df['age'].replace({0:1})"
      ],
      "metadata": {
        "id": "sv6smL0xKUa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cleaned construction year enables us to engineer a pump and the average population served per year feature."
      ],
      "metadata": {
        "id": "1pGguAwrUME-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #The cunstruction year should be a datetime data type\n",
        "# df['construction_year']=df['construction_year'].astype('datetime64[ns]')\n",
        "# df.dtypes"
      ],
      "metadata": {
        "id": "yZ9WviTXT9cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we impute the missing values with the string \"No Record\"\n",
        "df.scheme_name= df.scheme_name.fillna('No Record')\n",
        "df.scheme_management = df.scheme_management.fillna('No Record')\n",
        "df.installer = df.installer.fillna('No Record')\n",
        "df.funder = df.funder.fillna('No Record')\n",
        "df.public_meeting = df.public_meeting.fillna('No Record')\n",
        "df.permit = df.permit.fillna('No Record')\n",
        "df.subvillage = df.subvillage.fillna('No Record')"
      ],
      "metadata": {
        "id": "EfPYkTYUraP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "k40zvgtGxVqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### e) Consistency"
      ],
      "metadata": {
        "id": "y37uAYqixuq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicates\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "5mehferCx-HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No duplicate rows were found in our data set"
      ],
      "metadata": {
        "id": "TnudK5WOyIJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploratory Data Analysis**"
      ],
      "metadata": {
        "id": "6Pw5_OvPpNnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a)Univariate analysis."
      ],
      "metadata": {
        "id": "1RpUQqO0pe9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "gZqJSQ_5pa7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selecting object datatypes columns\n",
        "categorical = ['basin', 'region', \n",
        "         'public_meeting', 'recorded_by',\n",
        "       'scheme_management', 'permit',\n",
        "       'extraction_type_group', 'extraction_type_class',\n",
        "       'management', 'management_group',  'payment_type',\n",
        "        'quality_group', 'quantity_group',\n",
        "       'source', 'source_type', 'source_class', \n",
        "       'waterpoint_type_group']\n",
        "categorical\n",
        "\n",
        "# lets make a for loop to make countplots for our categorical variables.\n",
        "for col in categorical:\n",
        "  ax=sns.countplot(y=col,data=df)\n",
        "  plt.title(f\"countplot of {col}\")\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "O86DYJ2CrQml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b)Bivariate analysis"
      ],
      "metadata": {
        "id": "XotvMznCdaSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a crosstab \n",
        "crosstb=pd.crosstab(df.region,df.extraction_type_class)\n",
        "\n",
        "#creating a bar plot\n",
        "plt.figure(figsize=(34,30))\n",
        "pl=crosstb.plot(kind=\"bar\",stacked=True,rot=90)\n",
        "plt.title(\"extraction mode in each region\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BXMhJDM3dYD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot above shows region and most used mode of extraction on the water pumps\n",
        "\n"
      ],
      "metadata": {
        "id": "Tc4xyp8wdxaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a crosstab \n",
        "crosstb=pd.crosstab(df.region,df.payment_type)\n",
        "\n",
        "#creating a bar plot\n",
        "plt.figure(figsize=(34,30))\n",
        "pl=crosstb.plot(kind=\"bar\",stacked=True,rot=90)\n",
        "plt.title(\"payment criteria per region\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DnZHox2Ad42e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot above shows how pple pay for their water ,we can see that in Dar es salaam and mtwara payment per backet is almost more common than never pay ,meaning its harder to get free water than all other places."
      ],
      "metadata": {
        "id": "u3Ga3NpJeEL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a crosstab \n",
        "crosstb=pd.crosstab(df.region,df.source_type)\n",
        "\n",
        "#creating a bar plot\n",
        "plt.figure(figsize=(34,30))\n",
        "pl=crosstb.plot(kind=\"bar\",stacked=True,rot=90)\n",
        "plt.title(\"source type in each region\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P4cOT0EfeC_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Springs seem to be a commomn source across the regions followed by shallow well and river/lake and boreholes.\n",
        "The city Dar es salam seems to have very few water sources."
      ],
      "metadata": {
        "id": "ZjXI3cJIfQZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a crosstab \n",
        "crosstb=pd.crosstab(df.region,df.management_group)\n",
        "\n",
        "#creating a bar plot\n",
        "plt.figure(figsize=(34,30))\n",
        "pl=crosstb.plot(kind=\"bar\",stacked=True,rot=90)\n",
        "plt.title(\"management group in each region\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9ksAUDJrf1gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most pumps seem to be managed by the comunities that use them ."
      ],
      "metadata": {
        "id": "AA-PfkRxgWPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing Feature Engineering"
      ],
      "metadata": {
        "id": "QHwY1SRwTOnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for df\n",
        "df['water_/_person'] = df['amount_tsh'].replace({0:1}) / df['population'].replace({0:1})"
      ],
      "metadata": {
        "id": "FWCYn18mVMcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then write a function to check for the cardinality of each feature(how many unique values there are in the feature)"
      ],
      "metadata": {
        "id": "9MVBRxciVcZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse_cardinality_check(n, df):\n",
        "# this function will search the dataframe for features above the cardinality limit, \n",
        "# then create a dict from the results\n",
        "  \n",
        "  feature_list = []\n",
        "  \n",
        "  cardinality_value = []\n",
        "  \n",
        "  for _ in range(len(df.columns)):\n",
        "    if len(df[df.columns[_]].value_counts()) > n:\n",
        "      \n",
        "      feature_list.append(df.columns[_])\n",
        "      \n",
        "      cardinality_value.append(len(df[df.columns[_]].value_counts()))\n",
        "                               \n",
        "        \n",
        "  feature_dict = dict(zip(feature_list, cardinality_value))\n",
        "  \n",
        "  return feature_dict"
      ],
      "metadata": {
        "id": "cps1IN8jVtKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then preview our high cardinality features"
      ],
      "metadata": {
        "id": "7SHY3xQvWHtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "high_cardinality_feature_dict = reverse_cardinality_check(150, df)\n",
        "high_cardinality_feature_dict"
      ],
      "metadata": {
        "id": "Hp-JRBH9WCRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create dataframes for our high and low cardinality features"
      ],
      "metadata": {
        "id": "f7LWY1EFWV2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataframe for high cardinality\n",
        "high_cardinality_features = df[list(high_cardinality_feature_dict.keys())]\n",
        "high_cardinality_features.columns"
      ],
      "metadata": {
        "id": "tlXCxiFSWhcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataframe for low cardinality features\n",
        "low_cardinality_features = df.drop(columns = list(high_cardinality_feature_dict.keys()))\n",
        "low_cardinality_features.columns"
      ],
      "metadata": {
        "id": "Y2FHcvyHWsBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now perform label encoding for each dataframe"
      ],
      "metadata": {
        "id": "ZxULVo1AW5sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the numerical columns\n",
        "one_hot_encode = ce.OneHotEncoder(use_cat_names=True)\n",
        "one_hot_encode.fit(low_cardinality_features, df['status_group'])\n",
        "low_cardinality_features = one_hot_encode.transform(low_cardinality_features)\n",
        "\n",
        "ordinal_encode = ce.OrdinalEncoder()\n",
        "ordinal_encode.fit(high_cardinality_features, df['status_group'])\n",
        "high_cardinality_features = ordinal_encode.transform(high_cardinality_features)"
      ],
      "metadata": {
        "id": "ugsreiECW0cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_cardinality_features.isnull().sum()"
      ],
      "metadata": {
        "id": "2HNEo_kjze9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's drop missing values in the encoded dataframes\n",
        "low_cardinality_features.dropna().head()\n",
        "high_cardinality_features.dropna().head()"
      ],
      "metadata": {
        "id": "YE6k-KEnZ8Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have null values in the index columns"
      ],
      "metadata": {
        "id": "ojOhOIUuSGs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's merge the two encoded features back together"
      ],
      "metadata": {
        "id": "FhmpkboWXe9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# features = low_cardinality_features.concat(high_cardinality_features,\n",
        "#                                           on = low_cardinality_features.index)\n",
        "frames =[low_cardinality_features, high_cardinality_features]\n",
        "\n",
        "features = pd.concat(frames, axis = 1)"
      ],
      "metadata": {
        "id": "afhoqvAYXwG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# previewing the datatset\n",
        "features.head()"
      ],
      "metadata": {
        "id": "Dz0BOXPkUDid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we impute and scale our features"
      ],
      "metadata": {
        "id": "MpPHFZZEUk83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's impute using the mean\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer.fit(features, df['status_group'])\n",
        "features = imputer.transform(features)\n",
        "\n",
        "scaler = RobustScaler()\n",
        "scaler.fit(features, df['status_group'])\n",
        "features = scaler.transform(features)"
      ],
      "metadata": {
        "id": "igpampYVUthp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "T8JbWxFOfqa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's select our x and y variables\n",
        "X = df.drop('status_group', axis = 1).values\n",
        "y = df['status_group']\n",
        "\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "model = ExtraTreesClassifier()\n",
        "model.fit(X,y)\n",
        "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
        "#plot graph of feature importances for better visualization\n",
        "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "feat_importances.nlargest(10).plot(kind='barh')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w-xWM0OCfIey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's perform the operations above for our target dataframe as well(x Test)"
      ],
      "metadata": {
        "id": "ZKV_1F20V9U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # for high cardinality columns in the test dataset\n",
        "# high_cardinality_X_test_dict = reverse_cardinality_check(150, X_test)\n",
        "# high_cardinality_X_test_dict"
      ],
      "metadata": {
        "id": "WDKtTGRlWGsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# low_cardinality_X_test = X_test.drop(columns = list(high_cardinality_X_test_dict.keys()))\n",
        "# low_cardinality_X_test.columns"
      ],
      "metadata": {
        "id": "WoPDs5RuWYWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # for low cardinality columns in our test dataset\n",
        "# high_cardinality_X_test = X_test[list(high_cardinality_X_test_dict.keys())]\n",
        "# high_cardinality_X_test.columns"
      ],
      "metadata": {
        "id": "b0CyEZdmWhww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # encoding for low cardinality\n",
        "# low_cardinality_X_test = one_hot_encode.transform(low_cardinality_X_test)"
      ],
      "metadata": {
        "id": "4nxPw046WwlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # encoding for high cardinality\n",
        "# high_cardinality_X_test = ordinal_encode.transform(high_cardinality_X_test)"
      ],
      "metadata": {
        "id": "vv19dbt5W0Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # merging the encoded columns in our test data\n",
        "\n",
        "# frames_test =[low_cardinality_X_test, high_cardinality_X_test]\n",
        "\n",
        "# X_test = pd.concat(frames_test, axis = 1)"
      ],
      "metadata": {
        "id": "wFM5DHWRXE6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # imputing values on x test using the mean\n",
        "# X_test = imputer.transform(X_test)"
      ],
      "metadata": {
        "id": "DxRSaE--XfaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # scaling the x test\n",
        "# X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "6W_bpgneXnLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's Visualize our data"
      ],
      "metadata": {
        "id": "m9GWt-iXXsCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas"
      ],
      "metadata": {
        "id": "JOpCQ4r7Xuks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's make our imports\n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 30, 20\n",
        "import geopandas"
      ],
      "metadata": {
        "id": "Xg9P5MDTX10G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's visualize the data\n",
        "\n",
        "import geopandas\n",
        "\n",
        "gdf = geopandas.GeoDataFrame(df, geometry=geopandas.points_from_xy(df.longitude, df.latitude))\n",
        "\n",
        "functional = gdf.where(gdf['status_group'] == 'functional')\n",
        "repair = gdf.where(gdf['status_group'] == 'functional needs repair')\n",
        "broken = gdf.where(gdf['status_group'] == 'non functional')\n",
        "\n",
        "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
        "\n",
        "# We restrict to Africa\n",
        "ax = world[world.continent == 'Africa'].plot(\n",
        "    color='gray', edgecolor='black')\n",
        "\n",
        "ax.scatter(functional['longitude'], functional['latitude'],\n",
        "           c='green',alpha=.5, s=3)\n",
        "\n",
        "ax.scatter(repair['longitude'], repair['latitude'],\n",
        "           c='blue', alpha=.5, s=5)\n",
        "\n",
        "ax.scatter(broken['longitude'], broken['latitude'],\n",
        "           c='red', alpha=.5, s=5)\n",
        "plt.title(\"Map of Pump Distributions, Green-Functional, Blue-Repair, Red-Broken\", fontsize = 25)\n",
        "\n",
        "plt.ylim(-12, 0)\n",
        "plt.xlim(28,41)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iNlix0koX-K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelling"
      ],
      "metadata": {
        "id": "LmenOWz_f8Nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_matrix, y_vector = features, df['status_group']"
      ],
      "metadata": {
        "id": "lXe23adkf96B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "majority_class = y_vector.mode()\n",
        "y_vector.value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "CCHaSE74glAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "majority_prediction = [majority_class] * len(y_vector)\n",
        "accuracy_score(y_vector, majority_prediction)"
      ],
      "metadata": {
        "id": "xhEv9Chhg7wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "VlZYQrFUZHAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree Classifier"
      ],
      "metadata": {
        "id": "MYNmpdhthRmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's import decision trees classifier\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "X, y = make_classification(100, 5, n_classes = 2, shuffle = True, random_state= 10)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle = True, random_state=1)\n",
        "\n",
        "model = tree.DecisionTreeClassifier()\n",
        "model = model.fit(X_train, y_train)\n",
        "\n",
        "predicted_value = model.predict(X_test)\n",
        "print(predicted_value)\n",
        "#%%\n",
        "tree.plot_tree(model)\n",
        "\n",
        "zeroes = 0\n",
        "ones = 0\n",
        "for i in range(0,len(y_train)):\n",
        "    if y_train[i] == 0:\n",
        "        zeroes +=1\n",
        "    else:\n",
        "        ones +=1\n",
        "      \n",
        "print(zeroes)\n",
        "print(ones)\n",
        "\n",
        "val = 1 - ((zeroes/70)*2 + (ones/70)*2)\n",
        "print(\"Gini : -\",val)\n",
        " \n",
        "match = 0\n",
        "UnMatch = 0\n",
        " \n",
        "for i in range(30):\n",
        "    if predicted_value[i] == y_test[i]:\n",
        "        match += 1\n",
        "    else:\n",
        "        UnMatch += 1\n",
        "         \n",
        "accuracy = match/30\n",
        "print(\"Accuracy is: \",accuracy)"
      ],
      "metadata": {
        "id": "21AVJVU7Y-k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gini Index is a score that evaluates how accurate a split is among the classified groups. Gini index evaluates a score in the range between 0 and 1, where 0 is when all observations belong to one class, and 1 is a random distribution of the elements within classes. The gini score attained implies that the model is accurate with random distribution of values."
      ],
      "metadata": {
        "id": "Ay8zrJWDcEk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "decision_tree = DecisionTreeClassifier(max_depth=20)\n",
        "decision_tree.fit(X_matrix, y_vector)"
      ],
      "metadata": {
        "id": "XKWOIQPFhRFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate"
      ],
      "metadata": {
        "id": "sGlGNfnhhYCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def con_matrix_analysis(model):\n",
        "  x = model.predict(features)\n",
        "  y = y_vector\n",
        "  \n",
        "  print(classification_report(y, x,\n",
        "        target_names=['Functional', 'Needs Repair', 'Not-Functional']))\n",
        "\n",
        "  con_matrix = pd.DataFrame(confusion_matrix(y, x), \n",
        "             columns=['Predicted Functional', 'Predicted Needs Repair', 'Predicted Not-Functional'], \n",
        "             index=['Actual Functional', 'Actual Needs Repair', 'Actual Not-Functional'])\n",
        "                            \n",
        "  sns.heatmap(data=con_matrix, cmap='cool')\n",
        "  plt.show();\n",
        "  return con_matrix"
      ],
      "metadata": {
        "id": "CUEIhXOphbji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rcParams['figure.figsize'] = 15, 10"
      ],
      "metadata": {
        "id": "e_duZufhhfN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "con_matrix_analysis(decision_tree)"
      ],
      "metadata": {
        "id": "gMShnoNXhjfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Nearest neighbor"
      ],
      "metadata": {
        "id": "LKaE2hoJnWkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = features\n",
        "y_train = \n",
        "X_test = X_test\n",
        "y_test = "
      ],
      "metadata": {
        "id": "CPr6_vWFnfjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.head()"
      ],
      "metadata": {
        "id": "K7LbL941sHhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Eo3IYzwPpJF1"
      }
    }
  ]
}